---
title: "Homework 1 - Group G"
author: "Matilde Castelli - Dogan Demirbilek - Eros Fabrici"
#output: html_notebook
output:
  html_document:
    toc: true
    toc_depth: 2
    theme: united
    highlight: tango
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# DAAG: **Data Analysis and Graphics Using R**

## Exercise 4.

For the data frame ais (DAAG package)
- Use the function `str()` to get information on each of the columns. Determine  whether any of the columns hold missing values.

```{r}
library(DAAG)
for (col in names(ais)) {
  str(ais[col])
}
#second method
str(ais)

#first option
#subset(ais,is.na(ais)) #select for columns

#second option
if(any(is.na(ais))) {
  print("Some columns have Na Values")
}else { 
  print("No missing values")
}

```


- Make a table that shows the numbers of males and females for each different sport. In which sports is there a large imbalance (e.g., by a factor of more than 2:1) in the numbers of the two sexes?
  
```{r}
#?ais
# table of males anf females for each sport
t<-table(Sex=ais$sex, Sport=ais$sport)
t
#apply the function to all the columns of the table
title <- apply(t,2, max) > 2*apply(t,2, min)

#Sports with large imbalance
colnames(t)[title]

```

## Exercise 6.

Create a data frame called *Manitoba.lakes* that contains the lake’s elevation (in meters above sea level) and area (in square kilometers) as listed below. Assign the names of the lakes using the row.names() function.

```
				elevation 	area
Winnipeg 			217 	24387
Winnipegosis 		254 	5374
Manitoba 			248 	4624
SouthernIndian 		254 	2247
Cedar 				253 	1353
Island 				227 	1223
Gods 				178 	1151
Cross 				207 	755
Playgreen 			217 	657

```
```{r}
Manitoba.lakes <- data.frame(
    elevation = c(217, 254, 248, 254, 253, 227, 178, 207, 217),
    area = c(24387, 5374, 4624, 2247, 1353, 1223, 1151, 755, 657)
)
rows = c("Winnipeg", "Winnipegosis", "Manitoba", "SouthernIndian", "Cedar", "Island", "Gods", "Cross", "Playgreen")
row.names(Manitoba.lakes) <- rows
print(Manitoba.lakes)

```

(a) Use the following code to plot log2(area) versus elevation, adding labeling information (there is an extreme value of area that makes a logarithmic scale pretty much
essential):

```{r}
library(DAAG)
attach(Manitoba.lakes)
plot(log2(area) ~ elevation, pch=16, xlim=c(170,280))
# NB: Doubling the area increases log2(area) by 1.0
text(log2(area) ~ elevation,
labels=row.names(Manitoba.lakes), pos=4)
text(log2(area) ~ elevation, labels=area, pos=2)
title("Manitoba’s Largest Lakes")
detach(Manitoba.lakes)
```

Devise captions that explain the labeling on the points and on the y-axis. It will be necessary
to explain how distances on the scale relate to changes in area.

(b) Repeat the plot and associated labeling, now plotting area versus elevation, but
specifying log="y" in order to obtain a logarithmic y-scale. [Note: The log="y"
setting carries across to the subsequent text() commands. See Subsection 2.1.5 for an
example.]

```{r}
library(DAAG)
attach(Manitoba.lakes) 
plot(y = area, x = elevation, pch=16, xlim = c(170,260), log = "y")
# NB: 10 x area increases log(area) by 1.0
text(area ~ elevation,
     labels=row.names(Manitoba.lakes), pos=4)
text(area ~ elevation, labels=area, pos=2)
title("Manitoba’s Largest Lakes")
detach(Manitoba.lakes)
```




## Exercise 11.

Run the following code:

```{r}
gender <- factor(c(rep("female", 91), rep("male", 92)))
table(gender)
gender <- factor(gender, levels=c("male", "female"))
table(gender)
gender <- factor(gender, levels=c("Male", "female")) 
# Note the mistake: "Male" should be "male"
table(gender)
table(gender, exclude=NULL)
rm(gender) # Remove gender
```

Explain the output from the successive uses of `table()`.
The first output shows a table with the counts of each factor level present in `gender`, which is a variable generated from the function `factor` which in turn encodes a vector as a factor. In our case, the vector is created with the merge of the vectors of characters `rep("female", 91)` and `rep("male", 92)` (`rep(a,b)` is a function that simply generates a vector by replicating `a`, `b` times) by using the function `c()` that takes an  aribitrary element of arguments and concatenates them into a single vector.

The second shows the same output, where `gender` is reassigned by using `factor` function, but in a different way. The variable `gender` itself is passed as an argument together with another vector, `c("male", "female")`, which indicates the levels of the new factor. In this case `factor` function will take the first argument, i.e. `gender`, and transform each element in it that correspondes to one of the elements in the second argument, i.e. `"female"` and `"male"`, into a factor level with the same label (e.g. `"female"` is converted in factor level `female`), to `<NA>` (not available) otherwise.

In the third case, we are doing the same thing as steps as in the previous, but in the argument `levels` passed to `factors` there is a mistake: `c("Male", "female")` instead of `c("male", "female")`. Therefore, all the elements equal to `male` will be set as `<NA>` in the new factor. Consequently, `table(gender)` will count 0 for factor level `Male`.

Finally, `table(gender, exclude=NULL)` will show also the count of elements in `gender` that are marked as `<NA>`. The count was not shown in the third case due to the fact that `table`, by default, excludes all the elements that are `<NA>` or `<NaN>` (not a number). 

## Exercise 12.

Write a function that calculates the proportion of values in a vector x that exceed some value
cutoff.

1. Use the sequence of numbers 1, 2, . . . , 100 to check that this function gives the result that
   is expected.
2. Obtain the vector “ex01.36” from the Devore6 (or Devore7) package. These data give the times required for individuals to escape from an oil platform during a drill. Use dotplot() to show the distribution of times. Calculate the proportion of escape times that exceed 7 minutes.
```{r}
library(purrr) #lib for functional programming tools
library(Devore7)

#' Return the ration of elements in x that that exceed a cutoff value
#' @param x A vector/list of numeric values
#' @param cutoff A number 
#' @return Ratio of elements in x that exceed cutoff value
#' @examples
#' proportion_exceeding(1:50, 25) -> 0.5
proportion_exceeding <- function(x, cutoff){
  if(!is.numeric(x) || !is.numeric(cutoff)){
    stop("Both arguments have to be numeric!")
  }
  
  n <- x %>% length
  n_exceeding <- x %>% keep(function(el) el > cutoff) %>% length
  return(n_exceeding/n)
}
```
```{r}
proportion_exceeding(1:100, 99)
```
```{r}
data(ex01.36)
dotplot(C1 ~ 1:length(C1), data = ex01.36, ylim=0:450,
        horizontal = FALSE, xlab="Individuals", ylab="Time (seconds)")
```

```{r}
#proportion of the excape times that exceeds 7 minutes
proportion_exceeding(ex01.36$C1, 7*60) # ~3,85%
```

## Exercise 13.

The following plots four different transformations of the Animals data from the MASS package.
What different aspects of the data do these different graphs emphasize? Consider the effect on low values of the variables, as contrasted with the effect on high values.

```{r}
par(mfrow=c(2,2)) # 2 by 2 layout on the page
library(MASS) # Animals is in the MASS package

#body weight in kg, brain weight in g.
plot(brain ~  body, data=Animals)
plot(sqrt(brain) ~  sqrt(body), data=Animals)
plot(I(brain^0.1) ~  I(body^0.1), data=Animals)
# I() forces its argument to be treated "as is", ^ is treated as an arithmetic operator
plot(log(brain) ~ log(body), data=Animals)
par(mfrow=c(1,1)) # Restore to 1 figure per page
```

- The top left graph shows the original data,the weight of body in kg and the weight of brain in g. We can observe that the range of values is very wide and only few points are much larger than the rest of the data. This leads to uninformative and unclear representation, since the majority of the data are massed close to the origin and we can not retrieve any information about any possible relationship between the variables.
- In the top right graph the application of the square root transformation is not sufficient to minimize the effect of outliers, and the representation of data is not yet very clear.
- In the bottom left graph this transformation highlghts more clearly a sort of correlation between values. 
- In the bottom right log-log plot the effect of outliers is minimized, reducing wide range of values to a more manageable size. Data are equally spreaded and thus the correlation between the two weights is underlined.


## Exercise 15.

The data frame `socsupport` (DAAG) has data from a survey on social and other kinds of support, for a group of university students. It includes `Beck Depression Inventory` (BDI) scores.

The following are two alternative plots of BDI against age:

```R
plot(BDI ˜ age, data=socsupport)
plot(BDI ˜ unclass(age), data=socsupport)
```

- For examination of cases where the score seems very high, which plot is more useful? Explain.

Individual value plot is more useful for examination of cases. Both plots are useful to assess central tendency, variability and identify outliers. However box-plots present ranges of values based on quartiles and when sample size is too small, the quartile estimates might not be meaningful. Therefore it is more logical to use individual value plot to examine cases.

- Why is it necessary to be cautious in making anything of the plots for students in the three oldest age categories (25-30, 31-40, 40+)?

Three oldest age categories (25-30, 31-40, 40+) are also has the least number of observations and plots such as boxplots estimate the properties of those age categories while plottoing them. Those estimations will not be meaningful when sample size is too small. Therefore it is necessary to be cautious about those age categories.



## Exercise 17.

Given a vector x, the following demonstrates alternative ways to create a vector of numbers from 1 through n, where n is the length of the vector:

```
x <- c(8, 54, 534, 1630, 6611)
seq(1, length(x))
seq(along=x)
```


Now set `x <- NULL` and repeat each of the calculations `seq(1, length(x))` and `seq(along=x).` 

Which version of the calculation should be used in order to return a vector of length 0 in the event that the supplied argument is `NULL.`

The second, because `seq(1, length(x))` is interpreted as `seq(from = 1, to = 0)` therefore producing the vector `[1,0]`, while `seq(along=x)` should generate a sequence `1, 2, ..., lenght(x)` but as `length(x)` is equal to 0, it will return a vector of lenght 0.

```{r}
x <- NULL
seq(1, length(x))
seq(along=x)
```


## Exercise 20

**The help page for iris (type help(iris)) gives code that converts the data in iris3 (datasets package) to case-by-variable format, with column names “Sepal.Length”,“Sepal.Width”, “Petal.Length”, “Petal.Width”, and “Species”. Look up the help pages for the functions that are used, and make sure that you understand them. Then add annotation to this code that explains each step in the computation.**

```{r}

#dimnames_ retrieves names of row and columns of iris3. The output is:
#  1. NULL
#  2. Sepal L., Sepal W., Petal L., Petal W.
#  3. Setosa, Versicolor, Virginica
  
dni3 <- dimnames(iris3) 

# Creates a new dataframe
# the function 'aperm' permutates the dimensions of iris3 passing from [50,4,3] to [50,3,4]
ii <- data.frame(matrix(aperm(iris3, c(1,3,2)), ncol = 4,
                        # the names of columns are obtained from the string character dni3[[2]] 
                        #and 'sub' replaces the first occurence of L. with Length 
                        #and W. with Width, while the rows are unnamed 
                        dimnames = list(NULL, sub(" L.",".Length",
                                        sub(" W.",".Width", dni3[[2]])))),
  #'gl',we generate a new factor by specifying, the numbers of levels=3, number of replications=50
    Species = gl(3, 50, labels = sub("S", "s", sub("V", "v", dni3[[3]]))))
# Check if the elements of the new dataframe ii are equal to the original iris.
# The output is TRUE
all.equal(ii, iris) 
```

  
-----

# CS: **Core Statistics**

## Exercise 1.1

**Exponential random variable, X ≥ 0, has p.d.f. f(x) = λ exp(−λ^x).**

**1. Find the c.d.f. and the quantile function for X.**

To find c.d.f of $f(x)$, we need to integrate it from $0$ to $x$.

$$
f(x) = \lambda e^{-\lambda x} \\
F(x) = P(X \leq x) \\
     = \int_{0}^{x} f(w) \; dw \\
     = \int_{0}^{x} \lambda e^{-\lambda w} \; dw \\
     = [-\lambda e^{-\lambda w}]_{0}^{x} \\
     = 1 - e^{-\lambda x} \\
$$

The quantile function (inverse c.d.f)

$$
     1 - e^{-\lambda Q} = p \\
     Q(p; \lambda) = \frac{\ln(1-p)}{\lambda}
$$

**2. Find Pr(X < λ) and the median of X.**

By using c.d.f function that we found before
$Pr(X < λ) = 1 - e^{-\lambda^2}$

Median is
$$
Q(p = \frac{2}{4}; \lambda)\\
=-\ln(1/2)\lambda
$$

**3. Find the mean and variance of X.**
Mean
$$
E[X] = \int_{0}^{+\infty} xf(x) \; dx \\
     = \int_{0}^{+\infty} x\lambda e^{-\lambda x} \; dx  \\
     = \left[-x e^{-\lambda x}\right]_{0}^{\infty} + \int_{0}^{\infty}e^{-\lambda x}dx   \\
     = (0-0) + \left[\frac{-1}{\lambda}e^{-\lambda x} \right]_{0}^{\infty} \\
     = 0 + \left(0 + \frac{1}{\lambda}\right)
     = \frac{1}{\lambda}
$$
Variance
$$
E[X^2] = \int_{-\infty}^{+\infty} x^2f(x) \; dx \\
= \int_{0}^{+\infty} x^2\lambda e^{-\lambda x} \; dx \\
= \left[-x^2e^{-\lambda x}\right]_{0}^{\infty} + \int_{0}^{\infty}2xe^{-\lambda x}dx \\
= (0-0) + \left[\frac{-2}{\lambda}  x e^{-\lambda x}\right]_{0}^{\infty} + \frac{2}{\lambda} \int_{0}^{\infty}e^{-\lambda x}dx\\
= (0-0) +\frac{2}{\lambda} \left[\frac{-1}{\lambda}e^{-\lambda x}  \right]_{0}^{\infty}
= \frac{2}{\lambda^2}\\
Var[X] = E[X^2] - E[X]^2\\
We \space already \space know \space E[X] = \frac{1}{\lambda}\\
Var[X] = \frac{2}{\lambda^2} - \frac{1}{\lambda^2} = \frac{1}{\lambda^2}
$$



## Exercise 1.2

**Evaluate Pr(X < 0.5, Y < 0.5) if X and Y have joint p.d.f. (1.2).**


The given joint p.d.f. of $X,Y$ is:

$$f(x,y) = \begin{cases}x+\frac{3}{2}y^2 & \mbox{if }0<x<1 \mbox{ and } 0<y<1\\0 & \mbox{otherwise} \end{cases}$$
The joint pdf is the function $f(x,y)$ such that, if $\Omega$ is any region in the $x-y$ plane,
$$Pr\{(X,Y)\in \Omega\} = \int\int_{\Omega} f(x,y)\, dx \, dy$$ 
with $f(x,y) \geq 0$ and $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x,y)\, dx\, dy = 1$.

We want to evaluate the probability from the joint p.d.f, in the region $\Omega = \{(x,y)|0\le x \le 0.5, 0\le y \le0.5\}$. 
We have that:

$$
\begin{aligned}
Pr(X < 0.5, Y < 0.5) &= \int_{0}^{0.5}\int_{0}^{0.5} f(x,y) \,dx\,dy \\[5pt]
&= \int_{0}^{0.5}\int_{0}^{0.5} x+\frac{3}{2}y^2 \,dx\,dy \\[5pt]
&= \int_{0}^{0.5}[\frac{1}{2}x^2+\frac{3}{2}y^2x]_{x=0}^{x=0.5}\,dy \\[5pt]
&= \int_{0}^{0.5}\frac{1}{8}+\frac{3}{4}y^2\,dy \\[5pt]
&= [\frac{1}{8}y+\frac{1}{4}y^3]_{y=0}^{y=0.5} \\[5pt]
&= \frac{1}{16} + \frac{1}{32} = \frac{3}{32} = 0.09357
\end{aligned}
$$

## Exercise 1.6

Let X and Y be non-independent random variables, such that $var(X) = σ^2_x$, $var(Y ) = σ^2_y$ and $cov(X, Y ) = σ^2_{xy}$. Using the result from Section 1.6.2, find var(X + Y) and var(X − Y).

As $X$ $Y$ are dependent, we have that

$$
Var(X+Y) = Var(X) + Var(Y) + 2Cov(X,Y)\\
= \sigma^2_x + \sigma^2_y + 2\sigma^2_{xy}\\

Var(X-Y) = Var(X) + Var(Y) - 2Cov(X,Y)\\
= \sigma^2_x + \sigma^2_y - 2\sigma^2_{xy}
$$

## Exercise 1.8

If $log(X) ∼ N(μ, σ^2)$, find the p.d.f. of X.


$Let\space Y = log(X) \space so \space X = e^Y\\$
According to change of variable technique in p.d.f
$f_x(X) = f_y(v^{-1}(x))\times|(v^{-1})'(x)|$ where $v^{-1}(y)$ is inverse  function of $X$ which is $log(X)$

$$
f_x(x) = \frac{1}{x} \frac{1}{\sqrt{2\pi}\sigma} exp\left({\frac{-(log(x)- \mu)^2}{2\sigma^2}}\right)
$$




## Exercise 1.9

**Discrete random variable Y has a Poisson distribution with parameter λ if its p.d.f. is $f(y) = λ^ye^{−\lambda}/y!$, for $y = 0, 1,$ . . .**

**1. Find the moment generating function for Y (hint: the power series representation of the exponential function is useful).**

The moment generating function is defined as $M_X(s) = E(e^{sX})$, with $s$ real. Since the Poisson distribution is discrete we have that:

$$
\begin{aligned}
M_Y(s) = E(e^{sY}) &= \sum^\infty_{x=0}e^{sy} \frac{\lambda^y e^{−\lambda}}{y!} \\
&= e^{-\lambda} \sum^\infty_{x=0}\frac{(\lambda e^s)^y}{y!}
\end{aligned}
$$
Recalling the power series representation of the exponential function $\sum^\infty_{k=0}\frac{x^k}{k!} = e^x$, $M_Y(s)$ can be rewritten as:

$$M_Y(s) = e^{-\lambda}e^{\lambda e^s} = e^{\lambda(e^s - 1)}$$

**2. If $Y_1∼Poi(λ_1)$ and independently $Y_2 ∼ Poi(λ_2)$, deduce the distribution of $Y_1 + Y_2$, by employing a general property of m.g.f.s.**

Given that $Y_1$ and $Y_2$ are independent random variables, we can exploit the property: $M_{X + Y}(s) = M_{X}(s)M_{Y}(s)$  

So we have:
$$M_{Y_1 + Y_2}(s) = M_{Y_1}(s)M_{Y_2}(s) = e^{\lambda_1(e^s - 1)}e^{\lambda_2(e^s - 1)} = e^{ \lambda_1(e^s - 1)+\lambda_2(e^s - 1)} = e^{(\lambda_1 + \lambda_2)(e^s - 1)}$$
From this moment generating function we can deduce that $Y_1 + Y_2 \sim \mbox{Pois}(\lambda_1 + \lambda_2)$

**3. Making use of the previous result and the central limit theorem, deduce the normal approximation to the Poisson distribution.**

Consider n independent Poisson variables $X_1,X_2,...,X_n$ with mean $\lambda/n$, $X_i \sim Pois(\lambda/n) \text{ for } i=1,...,n$. In the previous point we show that the sum of independent Poisson r.v. is a Poisson random variable $Y= \sum_{i=1}^nX_i \sim Pois(\lambda)$ , where $\lambda = \sum_{i=1}^{n} \lambda_i$
 

$$Y \sim N(\mu = \lambda, \sigma^2 = \lambda)$$
Thus, if $\lambda$ is large the distribution $\mbox{Pois}(\lambda)$ gets approximated well by the distribution $N(\lambda, \lambda)$

**4. Confirm the previous result graphically, using R functions dpois, dnorm, plot or barplot and lines. Confirm that the approximation improves with increasing λ**

```{r}
lambda <- c(10, 25, 50, 75 ,100)
n <- 150
x <- 1:n

d<-c()

par(mfrow=c(1,2))
plot(x, dpois(x,10),xlim=c(0,n), ylim=c(0,0.2), type = "l",
     col = "green", xlab = "x", ylab = "y",  lwd = 3)

for(l in lambda){
    
    y1 = dpois(x,l)
    y2 = dnorm(x,l,sqrt(l))
    # investigate approximation as lambda increases
    d<-c(d,max(abs(y2-y1)))
   
    lines(x, y1,  col = "green", type = "h",  lwd = 3)
    lines(x, y2, col = "black", type = "l",  lwd = 2)
}
legend("topright", legend=c("Poisson distribution", "Normal distribution"), fill=c("green", "black"))

plot(lambda,d, main = "Approximation increasing lambda", cex.main=1.0, pch = 19, col="black", type = "o", xlab = "lambda", ylab = "abs(difference)" )
```


# LAB: **Laboratory**

## Exercise 1
**Write a function binomial(x,n,p) for the binomial distribution above, depending on parameters x,n,p, and test it with some prespecified values. Use the function choose() for the binomial coefficient.**
**Plot two binomials with n=20, and p=0.3,0.6 respectively.**


The r.v. $X$ that counts the number of successes has **binomial distribution** with probability
function
$$
{\rm Pr}(X = x) = \binom{n}{x} p^x \, (1-p)^{n-x} \, , \hspace{1cm} x = 0,\ldots,n\, .
$$

```{r}
mybinom <- function(x,n,p){
  return( choose( n,x )*( p^{x} )*( (1-p)^{n-x} ) )
}
 # Test with some prespecified values.
all.equal(mybinom(2,20,0.2), dbinom(2,20,0.2))
all.equal(mybinom(4,5,0.2), dbinom(4,5,0.2))

```

```{r}
# plot
par(mfrow=c(1,2),mar=c(4,4,2,1),oma=c(0,0.2,0.2,0), pty="s", pch = 16)
plot(0:20, mybinom(0:20, 20, 0.3), 
     xlab = "x", ylab = "f(x)", cex.lab=1.5, main="n=20, p = 0.3", cex.main=1.5, col = 'blue')
plot(0:20, mybinom(0:20, 20, 0.6), 
     xlab = "x", ylab = "f(x)", cex.lab=1.5, main="n=20, p = 0.6", cex.main=1.5, col = 'blue')
```

 
## Exercise 2
- Generate in $\mathsf{R}$ the same output, but using $\mathsf{rgeom()}$ for generating the random variables. *Hint*: generate $n$ times three geometric distribution $X_1,\ldots, X_3$ with $p=0.08$, store them in a matrix and compute then the sum $Y$. 

```{r}

matrix_mini <-  matrix(data = 0, nrow = 1000, ncol = 4)

for (j in 1:3) {
  matrix_mini[,j] <- rgeom(1000, 0.08)
};

matrix_mini[,4] <- rowSums(matrix_mini[,1:3])

hist(matrix_mini[,4],col="grey",
     main ="Histogram of Sum of Three Geometric Dist. Sample", xlim = c(0,1000))

```




## Exercise 3
- Show in $\mathsf{R}$, also graphically, that $\mbox{Gamma}(n/2, 1/2)$ coincides with a $\chi^{2}_{n}$.

$$
Gamma(x;\frac{n}{2}, \frac{1}{2}) = \frac{\bigg( \frac{1}{2} \bigg)^{\frac{1}{2}}}{\Gamma(\frac{n}{2})}x^{\frac{n}{2}-1}e^{-\frac{x}{2}}\\
= \frac{1}{2^{\frac{n}{2}}\Gamma(\frac{n}{2})} x^{\frac{n}{2}-1}e^{-\frac{n}{2}} = \chi^2_n(x)
$$

```{r}
d_of_freedom <- 1000; alpha <- d_of_freedom/2; beta <- 1/2;
curve(dchisq(x, d_of_freedom), col="red", xlim=c(800, 1200), lwd = 3, ylab = "Density")
curve(dgamma(x, alpha, beta), col="blue", xlim=c(800, 1200), lwd = 1, add = TRUE)
```

- Find the 5\% and the 95\% quantiles of a $\mbox{Gamma}(3,3)$. 

```{r}
qgamma(0.05, 3, 3)
```
```{r}
qgamma(0.95, 3, 3)
```


## Exercise 4

- Generate $n=1000$ values from a $\mbox{Beta}(5,2)$ and compute the sample mean and the sample variance.

```{r}
n <- 1000

sample <- rbeta(n,5,2)

mean <- mean(sample)
sprintf("Sample mean is: %s ", mean)

variance <- var(sample)
cat("Sample variance is ",variance)
#hist(sample, breaks=40, probability=TRUE)

```



## Exercise 5
- Analogously, show with a simple $\mathsf{R}$ function that a negative binomial distribution may be seen as a mixture between a Poisson and a Gamma. In symbols: $X|Y \sim \mathcal{P}(Y)$, $Y \sim \mbox{Gamma}(\alpha, \beta)$, then $X \sim \ldots$.

```{r}
nb_mixture <- function(df,n){
  lambda = rgamma(n,df,df)
  X = rpois(n,lambda)
  return(X)
}

df<-5
n<-10000

# Plot of the mixture distribution - PoissonGamma - in green:
pois_Gamma <- nb_mixture(df,n)
plot( density(pois_Gamma), col="green", lwd=1, type="h",
main="Negative Binomial distribution and Poisson-Gamma mixture",cex.main=1.0)

# Negative Binomial distribution superposition - in black, the probability is (alpha+1/alpha):
Nbinom <- c(rnbinom(n, size=df, prob=df/(df+1)))
lines( density(Nbinom), col="black", lwd=2 )
legend("topright", legend=c("Pois-Gamma Mixture", "Negative Binomial"), fill=c("green", "black"))
```


## Exercise 6

- Instead of using the built-in function $\mathsf{ecdf()}$, write your own $\mathsf{R}$ function for the empirical cumulative distribution function and reproduce the two plots above.

```{r}
library(purrr)
my_ecdf <-  function(sample, x=NULL){
  
  #sample <- sort(unique(sample))
  
  if(is.null(x)){
    ecdf_sample <- (1:length(sample)) / length(sample) 
    ecdf_sample
  } else {
    minors <- sample %>% keep(function(elem) elem <= x) %>% length
    minors / length(sample)
  }
}


set.seed(2)
par(mfrow=c(1,2))
n<-50
y<-rbeta(n, 3,4)
edf_beta <- sort(y)
tt<-seq(from=0, to=1, by=0.01)
plot(edf_beta, my_ecdf(edf_beta), main="ECDF and CDF: n=50",pch = 19,xlab = "x",ylab = "Fn(x)")
abline(h = c(0,1), col = "gray70", lty = 2)
lines(tt, pbeta(tt,3,4), col=2, lty=2, lwd=2)

n2<-500
y2<-rbeta(n2, 3,4)
edf_beta <- sort(y2)
tt<-seq(from=0, to=1, by=0.01)
plot(edf_beta, my_ecdf(edf_beta), main="ECDF and CDF: n=500",pch = 19,xlab = "x",ylab = "Fn(x)")
abline(h = c(0,1), col = "gray70", lty = 2)
lines(tt, pbeta(tt,3,4), col=2, lty=2, lwd=2)

```

## Exercise 7

Compare in $\mathsf{R}$ the assumption of normality for these samples:

- $y_1, \ldots, y_{100} \sim t_{\nu},$ with $\nu=5,20, 100$. What does it happens when the number of degrees of freedom $\nu$ increases?

```{r}
nu <- c(5,20,100)
n <- 100
y <- matrix(data = 0,nrow = length(nu), ncol = n)
for (row in 1:length(nu)) {
  y[row,] <- rt(100, df = nu[row])
}
for (i in 1:length(nu)) {
  print( paste("nu: ", nu[i], "; mean: ", mean(y[i,]), "; variance: ", var(y[i,]) ))  
}

par(mfrow=c(1,3))
for (i in 1:length(nu)) {
  qqplot(qt(ppoints(n), nu[i]),
         y[i,], 
         xlab="Normal(0,1) quantiles", ylab="Sample quantiles", 
         main = paste("Q-Q plot for t(", nu[i], "): n=100"))
  qqline(y[i,],
         distribution = qnorm,
         probs = c(0.05, 0.95), col=2)
}
```

From the qqplots it is possible to observe that by increasing the degrees of freedom the mean of the sample mean tends to 0. (??)

- $y_1, \ldots, y_{100} \sim \mbox{Cauchy}(0,1)$. Do you note something weird for the extremes quantiles? 

```{r}
n <- 100
y <- rcauchy(100)

print( paste("mean: ", mean(y), "; variance: ", var(y) ))  

qqplot(qcauchy(ppoints(n)),
       y, 
       xlab="Cauchy(0,1) quantiles", ylab="Sample quantiles", 
       main = paste("Q-Q plot for Cauchy(0,1): n=100"))
qqline(y,
       distribution = qnorm,
       probs = c(0.05, 0.95), col=2)
```

The extreme quantiles are very far from the Normal quantiles line and also from the central sample quantiles, in fact the sample variance is very high.

## Exercise 8

- Write a general $\mathsf{R}$ function for checking the validity of the central limit theorem. *Hint* The function will consist of two parameters: clt_function <- function($\mathsf{n}$, $\mathsf{distr}$), where the first one is the sample size and the second one is the kind of distribution from which you generate. Use plots for visualizing the results.

```{r}
clt_function <- function(n, distr){
  
  sample_means <- rowMeans(matrix(distr, n))
  
}

set.seed(42)
clt_pois <- clt_function(10000, rpois(1000*1000,5))
clt_gamma <- clt_function(10000,rgamma(1000*1000, shape = 4 ,rate = 0.5))


par(mfrow=c(2,2))

hist(rpois(1000*1000,5),main = "Histogram of Poisson Dist.")
hist(clt_pois,prob=TRUE, col="grey")
lines(density(clt_pois), col="red", lwd=2)

hist(rgamma(1000 * 1000, shape = 4 ,rate = 0.5), main = "Histogram of Gamma Dist.")
hist(clt_gamma,prob=TRUE, col="grey")
lines(density(clt_gamma), col="red", lwd=2)
```
